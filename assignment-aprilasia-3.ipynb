{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-06T03:32:13.709722Z","iopub.execute_input":"2021-11-06T03:32:13.711023Z","iopub.status.idle":"2021-11-06T03:32:13.723562Z","shell.execute_reply.started":"2021-11-06T03:32:13.710940Z","shell.execute_reply":"2021-11-06T03:32:13.722436Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport sklearn\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2021-11-06T03:32:13.725513Z","iopub.execute_input":"2021-11-06T03:32:13.726256Z","iopub.status.idle":"2021-11-06T03:32:13.733543Z","shell.execute_reply.started":"2021-11-06T03:32:13.726206Z","shell.execute_reply":"2021-11-06T03:32:13.732353Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"markdown","source":"# **QUESTION 1**","metadata":{}},{"cell_type":"markdown","source":"Importing the data, having initial look on the dataset and looking for null values.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/task-rapp/ingredient.csv', delimiter=',')\ndf.head()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-06T03:32:13.734959Z","iopub.execute_input":"2021-11-06T03:32:13.735319Z","iopub.status.idle":"2021-11-06T03:32:13.768150Z","shell.execute_reply.started":"2021-11-06T03:32:13.735270Z","shell.execute_reply":"2021-11-06T03:32:13.767045Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-06T03:32:13.769530Z","iopub.execute_input":"2021-11-06T03:32:13.769778Z","iopub.status.idle":"2021-11-06T03:32:13.779671Z","shell.execute_reply.started":"2021-11-06T03:32:13.769742Z","shell.execute_reply":"2021-11-06T03:32:13.778777Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"markdown","source":"The dataset is clear from null values, and proceed with further exploration","metadata":{}},{"cell_type":"markdown","source":"## **QUESTION 1a. (Descriptive Anlysis,Correlation and Anova)**","metadata":{}},{"cell_type":"markdown","source":"### **Descriptive Analysis**","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-06T03:32:13.781795Z","iopub.execute_input":"2021-11-06T03:32:13.782060Z","iopub.status.idle":"2021-11-06T03:32:13.830837Z","shell.execute_reply.started":"2021-11-06T03:32:13.782029Z","shell.execute_reply":"2021-11-06T03:32:13.829927Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"markdown","source":"**Findings:**\n* ingredient a, e have relatively low standard deviation compared to it's means *(Could be eliminated)*\n* ingredient a, b, d, e, g is a must-have in the formula\n* ingredient c, f ,h ,i are optionals since it has a minimum value of zero *(Couldn't be eliminated since it's important for clustering)*\n* some ingredients have higher means compared to the rest *(Normalized data needed)*\n\nTo see better on how each standard deviation interacts with each ingredient respective mean, I normalized the data by MinMax method","metadata":{}},{"cell_type":"code","source":"normalized_df=(df-df.min())/(df.max()-df.min())\nnormalized_df.describe()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-06T03:32:13.832996Z","iopub.execute_input":"2021-11-06T03:32:13.833468Z","iopub.status.idle":"2021-11-06T03:32:13.874217Z","shell.execute_reply.started":"2021-11-06T03:32:13.833314Z","shell.execute_reply":"2021-11-06T03:32:13.873285Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"markdown","source":"**Findings:**\n* h and i has high concentration in the near-zero value, eavluated from it's Q1, Q2 and Q3 value *(Might need some kind of transformation)* ","metadata":{}},{"cell_type":"markdown","source":"### **Correlation Analysis**","metadata":{}},{"cell_type":"code","source":"corr = normalized_df.corr()\ncorr.style.background_gradient(cmap='coolwarm')\ncorr.style.background_gradient(cmap='coolwarm').set_precision(2)\nmask = np.zeros_like(corr, dtype=bool)\nmask[np.triu_indices_from(mask)] = True\ncorr[mask] = np.nan\n(corr\n .style\n .background_gradient(cmap='coolwarm', axis=None, vmin=-1, vmax=1)\n .highlight_null(null_color='#f1f1f1')  # Color NaNs grey\n .set_precision(2))","metadata":{"execution":{"iopub.status.busy":"2021-11-06T03:32:13.875310Z","iopub.execute_input":"2021-11-06T03:32:13.875529Z","iopub.status.idle":"2021-11-06T03:32:13.928399Z","shell.execute_reply.started":"2021-11-06T03:32:13.875503Z","shell.execute_reply":"2021-11-06T03:32:13.927412Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"markdown","source":"**Findings:** \nFrom the correlation plot, I would like to classify each relations into some groups:\n* High Positive Correlation (>0,75):\n1. a and g\n* High Negative Correlation (<-0,75):\n1. None\n* Moderate Positive Correlation (0,25<x<0,75):\n1. b and h\n1. d and f\n1. d and h\n* Moderate Negative Correlation (-0,75<x<-0,25):\n1. a and d\n1. a and e\n1. a and f\n1. b and c\n1. b and f\n1. b and g\n1. c and d\n1. c and g\n1. c and h\n1. d and g\n* And the rest could be classified as uncorrelated\n\n\n* High negatively correlated ingredients might mean interchangeable subtances on the formula *(eventhough they're interchangeable they could be clustered into two different groups, so I chose to retain the data with this kind of correlation)*\n* High positively correlated ingredients might mean redundancy on the factors/variables and can be reduced to a single variable *(I eliminate a since it has high correlation with g, strengthening the previous argument to eliminate a)*\n","metadata":{}},{"cell_type":"markdown","source":"## **QUESTION 1b. (Graphical Analysis and Distribution Study)**","metadata":{}},{"cell_type":"markdown","source":"### **Graphical Analysis**","metadata":{}},{"cell_type":"markdown","source":"Making density plots and histograms of each variable","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(2, 5, figsize=(21, 7))\n\nsns.histplot(data=normalized_df, x=\"a\", kde=True,color=\"skyblue\", ax=axs[0, 0])\nsns.histplot(data=normalized_df, x=\"b\", kde=True,color=\"olive\", ax=axs[0, 1])\nsns.histplot(data=normalized_df, x=\"c\", kde=True,color=\"gold\", ax=axs[0, 2])\nsns.histplot(data=normalized_df, x=\"d\", kde=True,color=\"teal\", ax=axs[0, 3])\nsns.histplot(data=normalized_df, x=\"e\", kde=True,color=\"red\", ax=axs[0, 4])\nsns.histplot(data=normalized_df, x=\"f\", kde=True,color=\"green\", ax=axs[1, 0])\nsns.histplot(data=normalized_df, x=\"g\", kde=True,color=\"yellow\", ax=axs[1, 1])\nsns.histplot(data=normalized_df, x=\"h\", kde=True,color=\"pink\", ax=axs[1, 2])\nsns.histplot(data=normalized_df, x=\"i\", kde=True,color=\"blue\", ax=axs[1, 3])\n\nplt.show()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-06T03:32:13.929943Z","iopub.execute_input":"2021-11-06T03:32:13.930355Z","iopub.status.idle":"2021-11-06T03:32:15.865747Z","shell.execute_reply.started":"2021-11-06T03:32:13.930310Z","shell.execute_reply":"2021-11-06T03:32:15.864877Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"markdown","source":"Making boxplots of each variable","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(2, 5, figsize=(21, 7))\n\nsns.boxplot(data=normalized_df, y=\"a\",color=\"skyblue\", ax=axs[0, 0])\nsns.boxplot(data=normalized_df, y=\"b\",color=\"olive\", ax=axs[0, 1])\nsns.boxplot(data=normalized_df, y=\"c\",color=\"gold\", ax=axs[0, 2])\nsns.boxplot(data=normalized_df, y=\"d\",color=\"teal\", ax=axs[0, 3])\nsns.boxplot(data=normalized_df, y=\"e\",color=\"red\", ax=axs[0, 4])\nsns.boxplot(data=normalized_df, y=\"f\",color=\"green\", ax=axs[1, 0])\nsns.boxplot(data=normalized_df, y=\"g\",color=\"yellow\", ax=axs[1, 1])\nsns.boxplot(data=normalized_df, y=\"h\",color=\"pink\", ax=axs[1, 2])\nsns.boxplot(data=normalized_df, y=\"i\",color=\"blue\", ax=axs[1, 3])\n\nplt.show()\n","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-06T03:32:15.867886Z","iopub.execute_input":"2021-11-06T03:32:15.868165Z","iopub.status.idle":"2021-11-06T03:32:16.966258Z","shell.execute_reply.started":"2021-11-06T03:32:15.868133Z","shell.execute_reply":"2021-11-06T03:32:16.965378Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"markdown","source":"Making scatterplot matrix","metadata":{}},{"cell_type":"code","source":"sns.pairplot(normalized_df)","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-06T03:32:16.967464Z","iopub.execute_input":"2021-11-06T03:32:16.967719Z","iopub.status.idle":"2021-11-06T03:32:38.378087Z","shell.execute_reply.started":"2021-11-06T03:32:16.967687Z","shell.execute_reply":"2021-11-06T03:32:38.377348Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"markdown","source":"**Findings:** \n\nFrom the density and histogram plots\n* visually the ingredients a, b, d, and g are right-skewed\n* visually the ingredient e is left-skewed *(The skewed ones are fine but transformation is needed to reduce their tails)*\n* visually the ingredients h and i are zero-inflated *(This property is important in this problem so I let them be)*\n* visually the ingredient c and f is zero-inflated and bi-modal *(This property is important in this problem so I let them be)*\n\nFrom the boxplots and pair plots\n* almost all of the ingredients has notable outliers *(Getting rid of the outliers as it may affect the analysis. e.g. as in ingredient f where the bi-modality nature of the data wasn't showing due to outliers)*\n* in b >0.8 \n* in d >0.8\n* in f >0.4\n* in h >0.6\n* in i >0.8","metadata":{}},{"cell_type":"markdown","source":"## **QUESTION 1c. (Unsupervised Clustering)**","metadata":{}},{"cell_type":"markdown","source":"### **Transforming the Dataset**","metadata":{}},{"cell_type":"markdown","source":"From what we had explored on the previous questions, there some notable properties that we got:\n* some ingredients have higher means compared to the rest ->*Normalized dataset is needed to eliminate biases on distance measurements*\n* ingredient c, f ,h ,i are optionals since it has a minimum value of zero -> *This variables can't be reduced since the nature of dataset made zero inflated variable even more valuable*\n* ingredients a and e have a low standard deviation -> *they can be eliminated since there are no relevant changes in quantity*\n* a and g have high positive correlation -> *one of them could be eliminated to eliminate redundancy*\n* visually the ingredients h and i are zero-inflated  -> *this variables can't be reduced since the nature of dataset made zero inflated variable even more valuable*\n* visually the ingredient c and f are bi-modal and zero-inflated -> *no transformation needed since the nature of dataset made bi-modal variable even more valuable*\n* almost all of the ingredients had lots of outliers -> *this might be due to the tweaking of contents, reducing some of the data is needed to strengthen some characteritics that a feature has*\n\n","metadata":{}},{"cell_type":"markdown","source":"**Transformation that is going to be done:**\n* Eliminate ingredient a and e \n* Normalized the data by it's Min and Max value to retain the zero inflated-ness\n* Eliminate some of the data that's an outlier","metadata":{}},{"cell_type":"code","source":"# using normalized_df instead of df\n# eliminating ingredients a and e\nnormalized_df = normalized_df.drop(['a','e'],axis=1)\n# filtering the data from outliers\nnormalized_df = normalized_df[normalized_df['b'] <= 0.8]\nnormalized_df = normalized_df[normalized_df['d'] <= 0.8]\nnormalized_df = normalized_df[normalized_df['f'] <= 0.15]\nnormalized_df = normalized_df[normalized_df['g'] <= 0.8]\nnormalized_df = normalized_df[normalized_df['h'] <= 0.6]\nnormalized_df = normalized_df[normalized_df['i'] <= 0.8]","metadata":{"execution":{"iopub.status.busy":"2021-11-06T03:32:38.379454Z","iopub.execute_input":"2021-11-06T03:32:38.379679Z","iopub.status.idle":"2021-11-06T03:32:38.391741Z","shell.execute_reply.started":"2021-11-06T03:32:38.379653Z","shell.execute_reply":"2021-11-06T03:32:38.390767Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"code","source":"normalized_df=(normalized_df-normalized_df.min())/(normalized_df.max()-normalized_df.min())\nnormalized_df.info()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-06T03:32:38.393408Z","iopub.execute_input":"2021-11-06T03:32:38.393745Z","iopub.status.idle":"2021-11-06T03:32:38.423498Z","shell.execute_reply.started":"2021-11-06T03:32:38.393697Z","shell.execute_reply":"2021-11-06T03:32:38.422524Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(normalized_df)","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-06T03:32:38.425139Z","iopub.execute_input":"2021-11-06T03:32:38.425467Z","iopub.status.idle":"2021-11-06T03:32:51.549597Z","shell.execute_reply.started":"2021-11-06T03:32:38.425414Z","shell.execute_reply":"2021-11-06T03:32:51.548596Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"markdown","source":"**Findings after Transformation:** \nSeems like the dataset is where i wanted it to be, and I'll proceed with the clustering","metadata":{}},{"cell_type":"markdown","source":"### **Using K-Means Clustering and Elbow Method to find number of clusters**","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import KMeans\nfrom sklearn.preprocessing import MinMaxScaler\n\n\nx_array = np.array(normalized_df)\nscaler = MinMaxScaler()\nx_scaled = scaler.fit_transform(x_array)\n\ndistortions = []\nK = range(1,10)\nfor k in K:\n    kmeanModel = KMeans(n_clusters=k)\n    kmeanModel.fit(x_scaled)\n    distortions.append(kmeanModel.inertia_)\n\nplt.figure(figsize=(16,8))\nplt.plot(K, distortions, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Distortion')\nplt.title('The Elbow Method showing the optimal k')\nplt.show()\n","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-06T03:32:51.551165Z","iopub.execute_input":"2021-11-06T03:32:51.551439Z","iopub.status.idle":"2021-11-06T03:32:52.175848Z","shell.execute_reply.started":"2021-11-06T03:32:51.551407Z","shell.execute_reply":"2021-11-06T03:32:52.175180Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"markdown","source":"**Finding:** k=5 is visually being the elbow, I'll use 5 cluster","metadata":{}},{"cell_type":"markdown","source":"### **Using the Dendogram to get intuitive with the result**","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import normalize\ndata_scaled = normalize(normalized_df)\ndata_scaled = pd.DataFrame(data_scaled, columns=normalized_df.columns)\nimport scipy.cluster.hierarchy as shc\nplt.figure(figsize=(10, 7))  \nplt.title(\"Dendrograms\")  \ndend = shc.dendrogram(shc.linkage(data_scaled, method='ward'))\nplt.axhline(y=2, color='r', linestyle='--')","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-06T03:32:52.177077Z","iopub.execute_input":"2021-11-06T03:32:52.177904Z","iopub.status.idle":"2021-11-06T03:32:57.674070Z","shell.execute_reply.started":"2021-11-06T03:32:52.177871Z","shell.execute_reply":"2021-11-06T03:32:57.673175Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"markdown","source":"**Findings:** Seems like the dendogram goes hand in hand with the KMeans, I'll continue to group them into 5 clusters","metadata":{}},{"cell_type":"markdown","source":"### **Conclusion**","metadata":{}},{"cell_type":"markdown","source":"Finding the centroids of each cluster to have better look on what properties does each cluster has","metadata":{}},{"cell_type":"code","source":"kmeans = KMeans(n_clusters = 5, random_state=123)\nkmeans.fit(x_scaled)\ncc = kmeans.cluster_centers_\nprint(cc)","metadata":{"execution":{"iopub.status.busy":"2021-11-06T03:32:57.675263Z","iopub.execute_input":"2021-11-06T03:32:57.675512Z","iopub.status.idle":"2021-11-06T03:32:57.724778Z","shell.execute_reply.started":"2021-11-06T03:32:57.675482Z","shell.execute_reply":"2021-11-06T03:32:57.724180Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"code","source":"df_cc = pd.DataFrame(cc, columns=['b','c','d','f','g','h','i'])\ndf_cc","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-06T03:32:57.725889Z","iopub.execute_input":"2021-11-06T03:32:57.726158Z","iopub.status.idle":"2021-11-06T03:32:57.742499Z","shell.execute_reply.started":"2021-11-06T03:32:57.726127Z","shell.execute_reply":"2021-11-06T03:32:57.741494Z"},"trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"markdown","source":"**Final Findings:**\n\n","metadata":{}},{"cell_type":"markdown","source":"From the centroid data above, I classify each value qualitatively by each ingredients relative presence on the formula as \n* 0.30<= would be low, \n* 0.30< x <0.6 is moderate  \n* =>0.60 is High\n* and <=0.1 is absent\n\nAnd here is what the end result looks like","metadata":{}},{"cell_type":"code","source":"con = {'Ing.b':['Moderate','High','Moderate','Moderate','Moderate'],'Ing.c':['High','Absent','High','Absent','High'],'Ing.d':['Moderate','High','Low','Moderate','Moderate'],'Ing.f':['High','Absent','Low','Moderate','High'],'Ing.g':['Low','Low','Moderate','High','Low'],'Ing.h':['Absent','High','Absent','Absent','Absent'],'Ing.i':['Moderate','Absent','Low','Low','Absent']} \ncondf = pd.DataFrame(con)\ncondf","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-06T03:32:57.747308Z","iopub.execute_input":"2021-11-06T03:32:57.748123Z","iopub.status.idle":"2021-11-06T03:32:57.766361Z","shell.execute_reply.started":"2021-11-06T03:32:57.748069Z","shell.execute_reply":"2021-11-06T03:32:57.765318Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"markdown","source":"# **QUESTION 2**","metadata":{}},{"cell_type":"markdown","source":"Importing and early look at the data","metadata":{}},{"cell_type":"code","source":"df2 = pd.read_csv('/kaggle/input/task-rapp/palm_ffb.csv', delimiter=',')\ndf2.dataframeName = 'palm_ffb.csv'\nnRow, nCol = df2.shape\nprint(f'There are {nRow} rows and {nCol} columns')","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-06T03:32:57.767745Z","iopub.execute_input":"2021-11-06T03:32:57.768350Z","iopub.status.idle":"2021-11-06T03:32:57.783289Z","shell.execute_reply.started":"2021-11-06T03:32:57.768295Z","shell.execute_reply":"2021-11-06T03:32:57.782168Z"},"trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"code","source":"df2.head(5)","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-06T03:32:57.785095Z","iopub.execute_input":"2021-11-06T03:32:57.785936Z","iopub.status.idle":"2021-11-06T03:32:57.805183Z","shell.execute_reply.started":"2021-11-06T03:32:57.785891Z","shell.execute_reply":"2021-11-06T03:32:57.804544Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"code","source":"df2.isnull().sum()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-06T03:32:57.806306Z","iopub.execute_input":"2021-11-06T03:32:57.806651Z","iopub.status.idle":"2021-11-06T03:32:57.815117Z","shell.execute_reply.started":"2021-11-06T03:32:57.806623Z","shell.execute_reply":"2021-11-06T03:32:57.814251Z"},"trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"markdown","source":"**Findings:** No null values in the dataset, Since it's a yield data the date may affect the value of the yield due to seasonal changes. For this I then took the month of the year to replace the current date format","metadata":{}},{"cell_type":"markdown","source":"## **Descriptive and Graphical Analysis**","metadata":{}},{"cell_type":"markdown","source":"### Descriptive Analysis","metadata":{}},{"cell_type":"code","source":"df2.describe()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-06T03:32:57.816777Z","iopub.execute_input":"2021-11-06T03:32:57.817040Z","iopub.status.idle":"2021-11-06T03:32:57.854092Z","shell.execute_reply.started":"2021-11-06T03:32:57.816990Z","shell.execute_reply":"2021-11-06T03:32:57.853185Z"},"trusted":true},"execution_count":111,"outputs":[]},{"cell_type":"markdown","source":"### Graphical Analysis","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(2, 4, figsize=(21, 7))\n\nsns.histplot(data=df2, x=\"SoilMoisture\", kde=True,color=\"skyblue\", ax=axs[0, 0])\nsns.histplot(data=df2, x=\"Average_Temp\", kde=True,color=\"olive\", ax=axs[0, 1])\nsns.histplot(data=df2, x=\"Min_Temp\", kde=True,color=\"gold\", ax=axs[0, 2])\nsns.histplot(data=df2, x=\"Max_Temp\", kde=True,color=\"teal\", ax=axs[0, 3])\nsns.histplot(data=df2, x=\"Precipitation\", kde=True,color=\"red\", ax=axs[1, 0])\nsns.histplot(data=df2, x=\"Working_days\", kde=True,color=\"green\", ax=axs[1, 1])\nsns.histplot(data=df2, x=\"HA_Harvested\", kde=True,color=\"yellow\", ax=axs[1, 2])\nsns.histplot(data=df2, x=\"FFB_Yield\", kde=True,color=\"pink\", ax=axs[1, 3])\n\nplt.show()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-06T03:32:57.855195Z","iopub.execute_input":"2021-11-06T03:32:57.855404Z","iopub.status.idle":"2021-11-06T03:32:59.491377Z","shell.execute_reply.started":"2021-11-06T03:32:57.855378Z","shell.execute_reply":"2021-11-06T03:32:59.490364Z"},"trusted":true},"execution_count":112,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(2, 4, figsize=(21, 7))\n\nsns.boxplot(data=df2, y=\"SoilMoisture\",color=\"skyblue\", ax=axs[0, 0])\nsns.boxplot(data=df2, y=\"Average_Temp\",color=\"olive\", ax=axs[0, 1])\nsns.boxplot(data=df2, y=\"Min_Temp\",color=\"gold\", ax=axs[0, 2])\nsns.boxplot(data=df2, y=\"Max_Temp\",color=\"teal\", ax=axs[0, 3])\nsns.boxplot(data=df2, y=\"Precipitation\",color=\"red\", ax=axs[1, 0])\nsns.boxplot(data=df2, y=\"Working_days\",color=\"green\", ax=axs[1, 1])\nsns.boxplot(data=df2, y=\"HA_Harvested\",color=\"yellow\", ax=axs[1, 2])\nsns.boxplot(data=df2, y=\"FFB_Yield\",color=\"pink\", ax=axs[1, 3])\n\nplt.show()","metadata":{"_kg_hide-input":false,"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-06T03:32:59.492830Z","iopub.execute_input":"2021-11-06T03:32:59.493387Z","iopub.status.idle":"2021-11-06T03:33:01.096178Z","shell.execute_reply.started":"2021-11-06T03:32:59.493349Z","shell.execute_reply":"2021-11-06T03:33:01.095573Z"},"trusted":true},"execution_count":113,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(df2)","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-06T03:33:01.097100Z","iopub.execute_input":"2021-11-06T03:33:01.099336Z","iopub.status.idle":"2021-11-06T03:33:18.485048Z","shell.execute_reply.started":"2021-11-06T03:33:01.099291Z","shell.execute_reply":"2021-11-06T03:33:18.484432Z"},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"markdown","source":"**Findings:**\n* Most of variables except for FFB_Yield isn't really skewed *(Further transformation might not be needed)*\n* Precipitation, Min_Temp, Working_days and HA_Harvested have high Kurtosis *(Getting rid of the otliers that causes the kurtosis)*\n* Precipitation, Min_Temp, and HA_Harvested have noticable outliers *(Getting rid of the outliers)*\n* The features is suffering from collinearity or multi-collinearity *(Use VIF)*\n","metadata":{}},{"cell_type":"markdown","source":"### Corellation Plot","metadata":{}},{"cell_type":"code","source":"corr = df2.corr()\ncorr.style.background_gradient(cmap='coolwarm')\ncorr.style.background_gradient(cmap='coolwarm').set_precision(2)\nmask = np.zeros_like(corr, dtype=bool)\nmask[np.triu_indices_from(mask)] = True\ncorr[mask] = np.nan\n(corr\n .style\n .background_gradient(cmap='coolwarm', axis=None, vmin=-1, vmax=1)\n .highlight_null(null_color='#f1f1f1')  # Color NaNs grey\n .set_precision(2))","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-06T03:33:18.485916Z","iopub.execute_input":"2021-11-06T03:33:18.486144Z","iopub.status.idle":"2021-11-06T03:33:18.526120Z","shell.execute_reply.started":"2021-11-06T03:33:18.486114Z","shell.execute_reply":"2021-11-06T03:33:18.525294Z"},"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"markdown","source":"**Findings:** \nFrom the correlation plot, I would like to classify each relations into some groups:\n* High Positive Correlation (>0,75):\n1. Max_Temp and Average_Temp\n* High Negative Correlation (<-0,75):\n1. None\n* Moderate Positive Correlation (0,25<x<0,75):\n1. Precipitation and Soil Moisture\n1. HA_Harvested and Average_Temp\n1. FFB_Yield and Precipitation\n1. Precipitation and Min_Temp\n1. HA_Harvested and Max_Temp\n* Moderate Negative Correlation (-0,75<x<-0,25):\n1. Average_Temp and Soil_Moisture\n1. Soil_Moisture and Max_Temp\n1. Precipitation and Average_Temp\n1. Precipitation and Max_Temp\n1. HA_Harvested and Soil_Moisture\n1. HA_Harvested and Precipitation\n* And the rest could be classified as uncorrelated\n* The features is suffering from collinearity or multi-collinearity *(Use VIF)*","metadata":{}},{"cell_type":"markdown","source":"## **Feature Engineering**\n### Removing the outliers","metadata":{}},{"cell_type":"code","source":"#Removing Outliers\ndf2 = df2[df2['Min_Temp']>=19.8]\ndf2 = df2[df2['Precipitation']<=350]\ndf2 = df2[df2['HA_Harvested']>=700000]","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-06T03:33:18.527352Z","iopub.execute_input":"2021-11-06T03:33:18.528065Z","iopub.status.idle":"2021-11-06T03:33:18.535293Z","shell.execute_reply.started":"2021-11-06T03:33:18.528030Z","shell.execute_reply":"2021-11-06T03:33:18.534417Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"markdown","source":"Another quick look to the distplot to see whether the removal of outlliers helped","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(2, 4, figsize=(21, 7))\n\nsns.histplot(data=df2, x=\"SoilMoisture\", kde=True,color=\"skyblue\", ax=axs[0, 0])\nsns.histplot(data=df2, x=\"Average_Temp\", kde=True,color=\"olive\", ax=axs[0, 1])\nsns.histplot(data=df2, x=\"Min_Temp\", kde=True,color=\"gold\", ax=axs[0, 2])\nsns.histplot(data=df2, x=\"Max_Temp\", kde=True,color=\"teal\", ax=axs[0, 3])\nsns.histplot(data=df2, x=\"Precipitation\", kde=True,color=\"red\", ax=axs[1, 0])\nsns.histplot(data=df2, x=\"Working_days\", kde=True,color=\"green\", ax=axs[1, 1])\nsns.histplot(data=df2, x=\"HA_Harvested\", kde=True,color=\"yellow\", ax=axs[1, 2])\nsns.histplot(data=df2, x=\"FFB_Yield\", kde=True,color=\"pink\", ax=axs[1, 3])\n\nplt.show()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-06T03:33:18.536804Z","iopub.execute_input":"2021-11-06T03:33:18.537079Z","iopub.status.idle":"2021-11-06T03:33:20.239212Z","shell.execute_reply.started":"2021-11-06T03:33:18.537048Z","shell.execute_reply":"2021-11-06T03:33:20.238505Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"markdown","source":"It helped normalizing the data","metadata":{}},{"cell_type":"markdown","source":"### Calculating VIF Score","metadata":{}},{"cell_type":"markdown","source":"I then calculate the VIF value to make sure if there are any features that are heavily correlated to one or more features that might reduce the performance of the model","metadata":{}},{"cell_type":"code","source":"#Calculating VIF and making a Dataframe to visualize\nimport pandas as pd\nfrom sklearn.linear_model import LinearRegression\ndef calculate_vif(df, features):    \n    vif, tolerance = {}, {}\n    for feature in features:\n        X = [f for f in features if f != feature]        \n        X, y = df[X], df[feature]\n        r2 = LinearRegression().fit(X, y).score(X, y)                \n        \n   \n        tolerance[feature] = 1 - r2\n        vif[feature] = 1/(tolerance[feature])\n    return pd.DataFrame({'VIF': vif, 'Tolerance': tolerance})","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-06T03:33:20.240489Z","iopub.execute_input":"2021-11-06T03:33:20.241176Z","iopub.status.idle":"2021-11-06T03:33:20.248515Z","shell.execute_reply.started":"2021-11-06T03:33:20.241140Z","shell.execute_reply":"2021-11-06T03:33:20.247574Z"},"trusted":true},"execution_count":118,"outputs":[]},{"cell_type":"code","source":"calculate_vif(df2, features=['SoilMoisture','Average_Temp','Min_Temp','Max_Temp',\n                                'Precipitation','Working_days','HA_Harvested'])","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-06T03:33:20.250049Z","iopub.execute_input":"2021-11-06T03:33:20.250345Z","iopub.status.idle":"2021-11-06T03:33:20.305069Z","shell.execute_reply.started":"2021-11-06T03:33:20.250305Z","shell.execute_reply":"2021-11-06T03:33:20.304123Z"},"trusted":true},"execution_count":119,"outputs":[]},{"cell_type":"markdown","source":"**Findings:** The average temperature features is highly correlated to more than one other features in the dataset, and also the VIF value is pretty high. Hence, I remove this feature","metadata":{}},{"cell_type":"code","source":"calculate_vif(df2, features=['SoilMoisture','Min_Temp','Max_Temp',\n                                'Precipitation','Working_days','HA_Harvested'])","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-06T03:33:20.306415Z","iopub.execute_input":"2021-11-06T03:33:20.306664Z","iopub.status.idle":"2021-11-06T03:33:20.347568Z","shell.execute_reply.started":"2021-11-06T03:33:20.306634Z","shell.execute_reply":"2021-11-06T03:33:20.346601Z"},"trusted":true},"execution_count":120,"outputs":[]},{"cell_type":"markdown","source":"**Findings:**\nAlmost all of the features are now having relatively low VIF value compared to their initial values, for now I consider them as independent variables and could be used for further analysis ","metadata":{}},{"cell_type":"markdown","source":"## **Modelling**","metadata":{}},{"cell_type":"markdown","source":"### Linear Regression\n* Dropping Average_Temp\n* Normalizing the data by MinMax so there won't be any scalability issues with linear regression coefficients","metadata":{}},{"cell_type":"code","source":"df2_1 = df2.drop(\"Average_Temp\", axis=1)\ndf2_1 = df2_1.drop(\"Date\", axis=1)\nnormdf2_1 = (df2_1 - df2_1.min())/(df2_1.max()-df2_1.min())\nnormdf2_2 = normdf2_1\nnormdf2_1 = pd.concat([df2['Date'],normdf2_1],axis=1)\nnormdf2_1['Month'] = pd.DatetimeIndex(normdf2_1['Date']).day\nnormdf2_1 = normdf2_1.drop(\"Date\", axis=1)\nnormdf2_1=pd.get_dummies(normdf2_1, columns=[\"Month\"])","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-06T03:33:20.349176Z","iopub.execute_input":"2021-11-06T03:33:20.349470Z","iopub.status.idle":"2021-11-06T03:33:20.369449Z","shell.execute_reply.started":"2021-11-06T03:33:20.349438Z","shell.execute_reply":"2021-11-06T03:33:20.368737Z"},"trusted":true},"execution_count":121,"outputs":[]},{"cell_type":"markdown","source":"Making model, and fitting the data","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nX=normdf2_2.drop('FFB_Yield', axis=1)\ny=normdf2_2['FFB_Yield']\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.10, random_state=101)\nlin_reg = LinearRegression()\nlin_reg.fit(X_train, y_train)\n\nprint(\"The accuracy score of the model:\", lin_reg.score(X_test, y_test))","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-06T03:33:20.370842Z","iopub.execute_input":"2021-11-06T03:33:20.371683Z","iopub.status.idle":"2021-11-06T03:33:20.387541Z","shell.execute_reply.started":"2021-11-06T03:33:20.371633Z","shell.execute_reply":"2021-11-06T03:33:20.386368Z"},"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"markdown","source":"**Findings:**\nThe accuracy score is bad, I'll use other method of regression and use this result as a base","metadata":{}},{"cell_type":"markdown","source":"Finding feature importance from the linear regression model displaying it in table and graph","metadata":{}},{"cell_type":"code","source":"importance_lin_reg = lin_reg.coef_\n# summarize feature importance\nfeature_importances_lin_reg = pd.DataFrame(importance_lin_reg,index = X_train.columns,\n                                 columns=['importance']).sort_values('importance',ascending=False)\nfeature_importances_lin_reg","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-06T03:33:20.389031Z","iopub.execute_input":"2021-11-06T03:33:20.389349Z","iopub.status.idle":"2021-11-06T03:33:20.406885Z","shell.execute_reply.started":"2021-11-06T03:33:20.389306Z","shell.execute_reply":"2021-11-06T03:33:20.405993Z"},"trusted":true},"execution_count":123,"outputs":[]},{"cell_type":"code","source":"from matplotlib import pyplot\npyplot.bar(X_train.columns, importance_lin_reg)\npyplot.show()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-06T03:33:20.408311Z","iopub.execute_input":"2021-11-06T03:33:20.408742Z","iopub.status.idle":"2021-11-06T03:33:20.625033Z","shell.execute_reply.started":"2021-11-06T03:33:20.408700Z","shell.execute_reply":"2021-11-06T03:33:20.624379Z"},"trusted":true},"execution_count":124,"outputs":[]},{"cell_type":"markdown","source":"**Findings:**\n* The 3 features with highest importance are: Precipitation, HA_Harvested and SoilMoisture","metadata":{}},{"cell_type":"markdown","source":"### Random Forest Regressor","metadata":{}},{"cell_type":"markdown","source":"Random forest is well known to be pretty robust but tend to overfit. To cover for the lack of accuracy from the linear regression model, I'll use Random Forest Regressor\n\nSide note: I also added month as a feature since i believe that it might be useful","metadata":{}},{"cell_type":"code","source":"normdf2_1.head()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-06T03:33:20.626250Z","iopub.execute_input":"2021-11-06T03:33:20.626720Z","iopub.status.idle":"2021-11-06T03:33:20.647017Z","shell.execute_reply.started":"2021-11-06T03:33:20.626667Z","shell.execute_reply":"2021-11-06T03:33:20.645798Z"},"trusted":true},"execution_count":125,"outputs":[]},{"cell_type":"markdown","source":"Putting the importance score into a table and a graph","metadata":{}},{"cell_type":"code","source":"# random forest for feature importance on a regression problem\nfrom sklearn.ensemble import RandomForestRegressor\nfrom matplotlib import pyplot\n\nX1=normdf2_1.drop('FFB_Yield', axis=1)\ny1=normdf2_1['FFB_Yield']\n\nfrom sklearn.model_selection import train_test_split\nX_train1, X_test1, y_train1, y_test1 = train_test_split(X1,y1,test_size=0.30, random_state=101)\n\n\n# define the model\nrf = RandomForestRegressor()\n# fit the model\nmodel = rf.fit(X1, y1)\n# get importance\nimportance = rf.feature_importances_\n# summarize feature importance\nfeature_importances = pd.DataFrame(importance,index = X_train1.columns,\n                                 columns=['importance']).sort_values('importance',ascending=False)\nfeature_importances","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-06T03:33:20.648509Z","iopub.execute_input":"2021-11-06T03:33:20.648862Z","iopub.status.idle":"2021-11-06T03:33:20.882175Z","shell.execute_reply.started":"2021-11-06T03:33:20.648811Z","shell.execute_reply":"2021-11-06T03:33:20.881353Z"},"trusted":true},"execution_count":126,"outputs":[]},{"cell_type":"code","source":"pyplot.bar(X_train1.columns, importance)\npyplot.show()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-06T03:33:20.883352Z","iopub.execute_input":"2021-11-06T03:33:20.883590Z","iopub.status.idle":"2021-11-06T03:33:21.179177Z","shell.execute_reply.started":"2021-11-06T03:33:20.883562Z","shell.execute_reply":"2021-11-06T03:33:21.178506Z"},"trusted":true},"execution_count":127,"outputs":[]},{"cell_type":"code","source":"print (\"This model has an accuracy score of:\", model.score(X_test1, y_test1))","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-06T03:33:21.180252Z","iopub.execute_input":"2021-11-06T03:33:21.180755Z","iopub.status.idle":"2021-11-06T03:33:21.197711Z","shell.execute_reply.started":"2021-11-06T03:33:21.180720Z","shell.execute_reply":"2021-11-06T03:33:21.197051Z"},"trusted":true},"execution_count":128,"outputs":[]},{"cell_type":"markdown","source":"**Findings:**\n* The 3 features with highest importance are: Precipitation, HA_Harvested and SoilMoisture\n* Month of the year is also showing pretty interesting pattern as some months are really affecting the yields and some doesn't\n* for the top features that affects the yield, the Random Forest Regressor agrees with the Linear Model which is nice\n","metadata":{}},{"cell_type":"markdown","source":"## **Conclusion**\n**Final Findings:**\n* The 3 features with highest importance score: Precipitation, HA_Harvested and SoilMoisture\n* Precipitation is positively correlated with yield and the other two are negatively correlated\n* HA_Harvested being important is strange, since the FFB_Yield has a unit of Tonnes/HA","metadata":{}},{"cell_type":"markdown","source":"# **QUESTION 3**","metadata":{}},{"cell_type":"markdown","source":"## **Preliminary**","metadata":{}},{"cell_type":"markdown","source":"Putting the pharagraph into a string","metadata":{}},{"cell_type":"code","source":"string = \"\"\"As a term, data analytics predominantly refers to an assortment of applications, from basic business \nintelligence (BI), reporting and online analytical processing (OLAP) to various forms of advanced\nanalytics. In that sense, it's similar in nature to business analytics, another umbrella term for\napproaches to analyzing data -- with the difference that the latter is oriented to business uses, while\ndata analytics has a broader focus. The expansive view of the term isn't universal, though: In some\ncases, people use data analytics specifically to mean advanced analytics, treating BI as a separate\ncategory.  Data analytics initiatives can help businesses increase revenues, improve operational\nefficiency, optimize marketing campaigns and customer service efforts, respond more quickly to\nemerging market trends and gain a competitive edge over rivals -- all with the ultimate goal of\nboosting business performance. Depending on the particular application, the data that's analyzed\ncan consist of either historical records or new information that has been processed for real-time\nanalytics uses. In addition, it can come from a mix of internal systems and external data sources.  At\na high level, data analytics methodologies include exploratory data analysis (EDA), which aims to find\npatterns and relationships in data, and confirmatory data analysis (CDA), which applies statistical\ntechniques to determine whether hypotheses about a data set are true or false. EDA is often\ncompared to detective work, while CDA is akin to the work of a judge or jury during a court trial -- a\ndistinction first drawn by statistician John W. Tukey in his 1977 book Exploratory Data Analysis.  Data\nanalytics can also be separated into quantitative data analysis and qualitative data analysis. The\nformer involves analysis of numerical data with quantifiable variables that can be compared or\nmeasured statistically. The qualitative approach is more interpretive -- it focuses on understanding\nthe content of non-numerical data like text, images, audio and video, including common phrases,\nthemes and points of view.\"\"\"","metadata":{"execution":{"iopub.status.busy":"2021-11-06T03:33:21.198837Z","iopub.execute_input":"2021-11-06T03:33:21.199536Z","iopub.status.idle":"2021-11-06T03:33:21.205321Z","shell.execute_reply.started":"2021-11-06T03:33:21.199502Z","shell.execute_reply":"2021-11-06T03:33:21.204413Z"},"trusted":true},"execution_count":129,"outputs":[]},{"cell_type":"code","source":"string","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-06T03:33:21.206285Z","iopub.execute_input":"2021-11-06T03:33:21.206960Z","iopub.status.idle":"2021-11-06T03:33:21.224620Z","shell.execute_reply.started":"2021-11-06T03:33:21.206929Z","shell.execute_reply":"2021-11-06T03:33:21.223743Z"},"trusted":true},"execution_count":130,"outputs":[]},{"cell_type":"markdown","source":"Cleaning the string from unwanted characters, such as line-breaks (\\n), punctuation etc.","metadata":{}},{"cell_type":"code","source":"string = string.replace(\"\\n\",\" \")\nstring = string.replace(\"--\",\"\")\npunc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\nfor x in string:\n    if x in punc:\n        string = string.replace(x, \"\")\nstring ","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-11-06T03:33:21.226275Z","iopub.execute_input":"2021-11-06T03:33:21.226927Z","iopub.status.idle":"2021-11-06T03:33:21.239485Z","shell.execute_reply.started":"2021-11-06T03:33:21.226878Z","shell.execute_reply":"2021-11-06T03:33:21.238441Z"},"trusted":true},"execution_count":131,"outputs":[]},{"cell_type":"markdown","source":"Seems like we're good to go","metadata":{}},{"cell_type":"markdown","source":"## **Question 3a.**\ncounting number of words in the paragraph ","metadata":{}},{"cell_type":"code","source":"x = len(string.split())\nprint(\"Number of word in the paragraph:\", x)","metadata":{"execution":{"iopub.status.busy":"2021-11-06T03:33:21.241135Z","iopub.execute_input":"2021-11-06T03:33:21.241695Z","iopub.status.idle":"2021-11-06T03:33:21.252620Z","shell.execute_reply.started":"2021-11-06T03:33:21.241649Z","shell.execute_reply":"2021-11-06T03:33:21.251706Z"},"trusted":true},"execution_count":132,"outputs":[]},{"cell_type":"code","source":"y = string.count('data')\na = (y/x)\nprint(\"Probability of word 'data':\",\"{:.2%}\".format(a))","metadata":{"execution":{"iopub.status.busy":"2021-11-06T03:33:21.253853Z","iopub.execute_input":"2021-11-06T03:33:21.254498Z","iopub.status.idle":"2021-11-06T03:33:21.266124Z","shell.execute_reply.started":"2021-11-06T03:33:21.254446Z","shell.execute_reply":"2021-11-06T03:33:21.265290Z"},"trusted":true},"execution_count":133,"outputs":[]},{"cell_type":"markdown","source":"## **Question 3b.**","metadata":{}},{"cell_type":"code","source":"z=len(set(string.split()))\nb=(z/x)\nprint(\"Probability of unique word used in the paragraph:\",\"{:.2%}\".format(b))","metadata":{"execution":{"iopub.status.busy":"2021-11-06T03:33:21.271958Z","iopub.execute_input":"2021-11-06T03:33:21.272585Z","iopub.status.idle":"2021-11-06T03:33:21.278443Z","shell.execute_reply.started":"2021-11-06T03:33:21.272536Z","shell.execute_reply":"2021-11-06T03:33:21.277566Z"},"trusted":true},"execution_count":134,"outputs":[]},{"cell_type":"markdown","source":"## **Question 3c.**","metadata":{}},{"cell_type":"code","source":"c = string.count('data analytics')\nd = string.count('analytics')\ne = (c/d)\nprint(\"Probability of 'analytics' occuring after 'data':\",\"{:.2%}\".format(e))","metadata":{"execution":{"iopub.status.busy":"2021-11-06T03:33:21.279759Z","iopub.execute_input":"2021-11-06T03:33:21.280016Z","iopub.status.idle":"2021-11-06T03:33:21.294626Z","shell.execute_reply.started":"2021-11-06T03:33:21.279963Z","shell.execute_reply":"2021-11-06T03:33:21.293700Z"},"trusted":true},"execution_count":135,"outputs":[]}]}